{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Process Rosbags for Training Data\n",
        "\n",
        "**RAPP Lab 04 - Vision-Action Model**\n",
        "\n",
        "This notebook processes rosbag recordings to extract and prepare training data:\n",
        "\n",
        "1. **Extract Topics:** Load skeleton tracking and joint state data\n",
        "2. **Synchronize:** Align messages by timestamp (interpolate joint states to skeleton rate)\n",
        "3. **Interactive Selection:** Choose which skeleton is the \"leader\" with 3D visualization\n",
        "4. **Transform:** Convert skeleton to robot_base_link coordinate frame\n",
        "5. **Quality Checks:** Validate tracking confidence, detect position jumps, check joint limits\n",
        "6. **Export CSV:** Save synchronized data for training\n",
        "7. **Update Metadata:** Track processed recordings\n",
        "\n",
        "---\n",
        "\n",
        "**MANUAL CONFIGURATION:** Set the rosbag name below for each processing run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MANUAL CONFIGURATION - Change this for each rosbag\n",
        "# =============================================================================\n",
        "\n",
        "ROSBAG_NAME = '25_12_11_RAPP_M_R2G1S1_01'  # <-- CHANGE THIS FOR EACH ROSBAG\n",
        "\n",
        "# =============================================================================\n",
        "# Do not modify below this line\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure vam_utils is importable\n",
        "workspace_dir = Path('/workspace')\n",
        "if str(workspace_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(workspace_dir))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from typing import Optional, Tuple, List, Dict, Any\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "\n",
        "# ROS2 message deserialization\n",
        "from rclpy.serialization import deserialize_message\n",
        "from sensor_msgs.msg import JointState\n",
        "from builtin_interfaces.msg import Time\n",
        "\n",
        "# For skeleton messages - we'll need to handle this\n",
        "import importlib.util\n",
        "\n",
        "# Visualization\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Load URDF for FK\n",
        "from urdf_parser_py.urdf import URDF\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration paths\n",
        "ROSBAG_DIR = Path('/data/rosbags')\n",
        "PROCESSED_DIR = Path('/data/processed')\n",
        "CONFIG_DIR = Path('/config')\n",
        "\n",
        "# Construct full path to rosbag database\n",
        "rosbag_path = ROSBAG_DIR / ROSBAG_NAME\n",
        "db_file = list(rosbag_path.glob('*.db3'))[0] if rosbag_path.exists() else None\n",
        "\n",
        "# Output paths\n",
        "csv_output_dir = PROCESSED_DIR / 'csv'\n",
        "csv_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "metadata_file = PROCESSED_DIR / 'recordings_metadata.csv'\n",
        "\n",
        "print(f\"Rosbag directory: {rosbag_path}\")\n",
        "print(f\"Database file: {db_file}\")\n",
        "print(f\"CSV output directory: {csv_output_dir}\")\n",
        "print(f\"Metadata file: {metadata_file}\")\n",
        "\n",
        "# Verify paths\n",
        "assert rosbag_path.exists(), f\"Rosbag directory not found: {rosbag_path}\"\n",
        "assert db_file is not None and db_file.exists(), f\"No .db3 file found in {rosbag_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_topic_messages_with_timestamps(db_path: Path, topic_name: str) -> List[Tuple[int, bytes]]:\n",
        "    \"\"\"\n",
        "    Extract raw serialized messages with timestamps from a rosbag database.\n",
        "    \n",
        "    Args:\n",
        "        db_path: Path to the .db3 file\n",
        "        topic_name: The ROS topic to extract\n",
        "        \n",
        "    Returns:\n",
        "        List of (timestamp_ns, serialized_message) tuples\n",
        "    \"\"\"\n",
        "    conn = sqlite3.connect(str(db_path))\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # Get topic ID\n",
        "    cursor.execute(\"SELECT id FROM topics WHERE name = ?\", (topic_name,))\n",
        "    result = cursor.fetchone()\n",
        "    if result is None:\n",
        "        conn.close()\n",
        "        raise ValueError(f\"Topic '{topic_name}' not found in rosbag\")\n",
        "    \n",
        "    topic_id = result[0]\n",
        "    \n",
        "    # Get messages with timestamps\n",
        "    cursor.execute(\n",
        "        \"SELECT timestamp, data FROM messages WHERE topic_id = ? ORDER BY timestamp\",\n",
        "        (topic_id,)\n",
        "    )\n",
        "    messages = cursor.fetchall()\n",
        "    \n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "\n",
        "def list_topics(db_path: Path) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"List all topics in a rosbag with their message counts and types.\"\"\"\n",
        "    conn = sqlite3.connect(str(db_path))\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT t.name, t.type, COUNT(m.id) as msg_count\n",
        "        FROM topics t\n",
        "        LEFT JOIN messages m ON t.id = m.topic_id\n",
        "        GROUP BY t.id\n",
        "    \"\"\")\n",
        "    \n",
        "    topics = {}\n",
        "    for name, msg_type, count in cursor.fetchall():\n",
        "        topics[name] = {'type': msg_type, 'count': count}\n",
        "    \n",
        "    conn.close()\n",
        "    return topics\n",
        "\n",
        "\n",
        "def timestamp_to_seconds(timestamp_ns: int) -> float:\n",
        "    \"\"\"Convert nanosecond timestamp to seconds (float).\"\"\"\n",
        "    return timestamp_ns / 1e9\n",
        "\n",
        "\n",
        "# List available topics\n",
        "topics = list_topics(db_file)\n",
        "print(\"\\nAvailable topics:\")\n",
        "for name, info in sorted(topics.items()):\n",
        "    print(f\"  {name}: {info['type']} ({info['count']} messages)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load URDF and Forward Kinematics\n",
        "\n",
        "Reuse the forward kinematics implementation from Phase 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load URDF\n",
        "urdf_path = PROCESSED_DIR / 'ur10.urdf'\n",
        "assert urdf_path.exists(), f\"URDF not found at {urdf_path}. Run 00_setup_urdf.ipynb first.\"\n",
        "\n",
        "with open(urdf_path, 'r') as f:\n",
        "    urdf_string = f.read()\n",
        "\n",
        "robot = URDF.from_xml_string(urdf_string)\n",
        "\n",
        "print(f\"Loaded URDF: {robot.name}\")\n",
        "print(f\"Joints: {len([j for j in robot.joints if j.type in ['revolute', 'continuous']])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy FK class from Phase 1 (simplified version)\n",
        "\n",
        "def quaternion_to_rotation_matrix(q: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Convert quaternion (xyzw) to 3x3 rotation matrix.\"\"\"\n",
        "    x, y, z, w = q\n",
        "    norm = np.sqrt(x*x + y*y + z*z + w*w)\n",
        "    x, y, z, w = x/norm, y/norm, z/norm, w/norm\n",
        "    \n",
        "    return np.array([\n",
        "        [1 - 2*(y*y + z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
        "        [2*(x*y + z*w), 1 - 2*(x*x + z*z), 2*(y*z - x*w)],\n",
        "        [2*(x*z - y*w), 2*(y*z + x*w), 1 - 2*(x*x + y*y)]\n",
        "    ])\n",
        "\n",
        "\n",
        "def rpy_to_rotation_matrix(rpy: Tuple[float, float, float]) -> np.ndarray:\n",
        "    \"\"\"Convert roll-pitch-yaw angles to 3x3 rotation matrix.\"\"\"\n",
        "    roll, pitch, yaw = rpy\n",
        "    cr, sr = np.cos(roll), np.sin(roll)\n",
        "    cp, sp = np.cos(pitch), np.sin(pitch)\n",
        "    cy, sy = np.cos(yaw), np.sin(yaw)\n",
        "    \n",
        "    return np.array([\n",
        "        [cy*cp, cy*sp*sr - sy*cr, cy*sp*cr + sy*sr],\n",
        "        [sy*cp, sy*sp*sr + cy*cr, sy*sp*cr - cy*sr],\n",
        "        [-sp, cp*sr, cp*cr]\n",
        "    ])\n",
        "\n",
        "\n",
        "def rotation_matrix_from_axis_angle(axis: np.ndarray, angle: float) -> np.ndarray:\n",
        "    \"\"\"Create rotation matrix from axis-angle representation.\"\"\"\n",
        "    axis = np.array(axis) / np.linalg.norm(axis)\n",
        "    K = np.array([\n",
        "        [0, -axis[2], axis[1]],\n",
        "        [axis[2], 0, -axis[0]],\n",
        "        [-axis[1], axis[0], 0]\n",
        "    ])\n",
        "    return np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)\n",
        "\n",
        "\n",
        "class UR10ForwardKinematics:\n",
        "    \"\"\"Forward kinematics for UR10 robot.\"\"\"\n",
        "    \n",
        "    def __init__(self, urdf: URDF):\n",
        "        self.urdf = urdf\n",
        "        self.joint_names = []\n",
        "        self.joint_info = {}\n",
        "        \n",
        "        for joint in urdf.joints:\n",
        "            if joint.type in ['revolute', 'continuous']:\n",
        "                self.joint_names.append(joint.name)\n",
        "                self.joint_info[joint.name] = {\n",
        "                    'parent': joint.parent,\n",
        "                    'child': joint.child,\n",
        "                    'origin_xyz': joint.origin.xyz if joint.origin else [0, 0, 0],\n",
        "                    'origin_rpy': joint.origin.rpy if joint.origin else [0, 0, 0],\n",
        "                    'axis': joint.axis if joint.axis else [0, 0, 1],\n",
        "                    'limits': (joint.limit.lower, joint.limit.upper) if joint.limit else (-np.pi, np.pi)\n",
        "                }\n",
        "    \n",
        "    def compute_transforms(self, joint_angles: Dict[str, float]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Compute 4x4 homogeneous transforms for all links.\"\"\"\n",
        "        transforms = {}\n",
        "        base_link = self.urdf.get_root()\n",
        "        transforms[base_link] = np.eye(4)\n",
        "        \n",
        "        for joint in self.urdf.joints:\n",
        "            parent = joint.parent\n",
        "            child = joint.child\n",
        "            \n",
        "            if parent not in transforms:\n",
        "                continue\n",
        "            \n",
        "            T = np.eye(4)\n",
        "            if joint.origin:\n",
        "                T[:3, 3] = joint.origin.xyz\n",
        "                T[:3, :3] = rpy_to_rotation_matrix(joint.origin.rpy)\n",
        "            \n",
        "            if joint.type in ['revolute', 'continuous']:\n",
        "                angle = joint_angles.get(joint.name, 0.0)\n",
        "                axis = joint.axis if joint.axis else [0, 0, 1]\n",
        "                R_joint = rotation_matrix_from_axis_angle(axis, angle)\n",
        "                T[:3, :3] = T[:3, :3] @ R_joint\n",
        "            \n",
        "            transforms[child] = transforms[parent] @ T\n",
        "        \n",
        "        return transforms\n",
        "    \n",
        "    def get_joint_positions(self, joint_angles: Dict[str, float]) -> List[np.ndarray]:\n",
        "        \"\"\"Get positions of joint origins.\"\"\"\n",
        "        transforms = self.compute_transforms(joint_angles)\n",
        "        positions = []\n",
        "        \n",
        "        base = self.urdf.get_root()\n",
        "        positions.append(transforms[base][:3, 3])\n",
        "        \n",
        "        for joint_name in self.joint_names:\n",
        "            child = self.joint_info[joint_name]['child']\n",
        "            if child in transforms:\n",
        "                positions.append(transforms[child][:3, 3])\n",
        "        \n",
        "        return positions\n",
        "\n",
        "\n",
        "fk = UR10ForwardKinematics(robot)\n",
        "print(f\"\\nInitialized FK with {len(fk.joint_names)} joints\")\n",
        "print(f\"Joint order: {fk.joint_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Joint States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract joint state messages with timestamps\n",
        "print(\"Extracting joint states...\")\n",
        "joint_state_data = get_topic_messages_with_timestamps(db_file, '/joint_states')\n",
        "\n",
        "print(f\"Found {len(joint_state_data)} joint state messages\")\n",
        "\n",
        "# Parse joint states into DataFrame\n",
        "joint_records = []\n",
        "for timestamp_ns, msg_bytes in joint_state_data:\n",
        "    msg = deserialize_message(msg_bytes, JointState)\n",
        "    \n",
        "    # Create joint angle dictionary\n",
        "    joint_angles = {name: pos for name, pos in zip(msg.name, msg.position)}\n",
        "    \n",
        "    # Map to URDF joint names in order\n",
        "    ordered_angles = []\n",
        "    for urdf_joint_name in fk.joint_names:\n",
        "        # Try direct match or partial match\n",
        "        angle = None\n",
        "        for js_name, js_angle in joint_angles.items():\n",
        "            if urdf_joint_name == js_name or urdf_joint_name in js_name or js_name in urdf_joint_name:\n",
        "                angle = js_angle\n",
        "                break\n",
        "        ordered_angles.append(angle if angle is not None else 0.0)\n",
        "    \n",
        "    joint_records.append({\n",
        "        'timestamp': timestamp_to_seconds(timestamp_ns),\n",
        "        'j0': ordered_angles[0],\n",
        "        'j1': ordered_angles[1],\n",
        "        'j2': ordered_angles[2],\n",
        "        'j3': ordered_angles[3],\n",
        "        'j4': ordered_angles[4],\n",
        "        'j5': ordered_angles[5]\n",
        "    })\n",
        "\n",
        "df_joints = pd.DataFrame(joint_records)\n",
        "print(f\"\\nJoint states DataFrame shape: {df_joints.shape}\")\n",
        "print(f\"Time range: {df_joints['timestamp'].min():.2f}s to {df_joints['timestamp'].max():.2f}s\")\n",
        "print(f\"Duration: {df_joints['timestamp'].max() - df_joints['timestamp'].min():.2f}s\")\n",
        "print(f\"Average frequency: {len(df_joints) / (df_joints['timestamp'].max() - df_joints['timestamp'].min()):.1f} Hz\")\n",
        "\n",
        "df_joints.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extract Skeleton Data\n",
        "\n",
        "**Note:** We need to handle ZED skeleton messages. Since `zed_msgs` might not be directly available,\n",
        "we'll parse the raw message data. The skeleton message contains an array of detected skeletons,\n",
        "each with 16 keypoints (x, y, z) and confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to import zed_msgs, if not available we'll parse manually\n",
        "try:\n",
        "    from zed_msgs.msg import ObjectsStamped\n",
        "    ZED_MSGS_AVAILABLE = True\n",
        "    print(\"zed_msgs available\")\n",
        "except ImportError:\n",
        "    ZED_MSGS_AVAILABLE = False\n",
        "    print(\"Warning: zed_msgs not available, will attempt manual parsing\")\n",
        "    print(\"Skeleton extraction may be limited\")\n",
        "\n",
        "# Extract skeleton messages\n",
        "print(\"\\nExtracting skeleton data...\")\n",
        "skeleton_data = get_topic_messages_with_timestamps(db_file, '/zed/zed_node/body_trk/skeletons')\n",
        "print(f\"Found {len(skeleton_data)} skeleton messages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse skeleton data\n",
        "skeleton_records = []\n",
        "\n",
        "if ZED_MSGS_AVAILABLE:\n",
        "    for timestamp_ns, msg_bytes in skeleton_data:\n",
        "        msg = deserialize_message(msg_bytes, ObjectsStamped)\n",
        "        \n",
        "        # Process each detected skeleton\n",
        "        for skeleton_obj in msg.objects:\n",
        "            # Extract 16 keypoints (ZED body tracking 16-point mode)\n",
        "            keypoints = skeleton_obj.skeleton_2d.keypoints  # or skeleton_3d depending on what's available\n",
        "            \n",
        "            # Note: We want 3D positions - check if skeleton_3d is available\n",
        "            if hasattr(skeleton_obj, 'skeleton_3d'):\n",
        "                keypoints = skeleton_obj.skeleton_3d.keypoints\n",
        "            \n",
        "            # Build record\n",
        "            record = {\n",
        "                'timestamp': timestamp_to_seconds(timestamp_ns),\n",
        "                'skeleton_id': skeleton_obj.label_id  # 0 or 1 typically\n",
        "            }\n",
        "            \n",
        "            # Add all 16 keypoints (x, y, z)\n",
        "            for i, kp in enumerate(keypoints[:16]):  # Ensure only 16 keypoints\n",
        "                record[f'sk_{i}_x'] = kp.kp[0]\n",
        "                record[f'sk_{i}_y'] = kp.kp[1]\n",
        "                record[f'sk_{i}_z'] = kp.kp[2]\n",
        "            \n",
        "            skeleton_records.append(record)\n",
        "else:\n",
        "    # Manual parsing fallback - this is complex and depends on message structure\n",
        "    print(\"ERROR: Manual skeleton parsing not yet implemented\")\n",
        "    print(\"Please ensure zed_msgs is available in the Docker environment\")\n",
        "    raise NotImplementedError(\"zed_msgs required for skeleton extraction\")\n",
        "\n",
        "df_skeletons = pd.DataFrame(skeleton_records)\n",
        "print(f\"\\nSkeleton DataFrame shape: {df_skeletons.shape}\")\n",
        "print(f\"Unique skeleton IDs: {df_skeletons['skeleton_id'].unique()}\")\n",
        "print(f\"Time range: {df_skeletons['timestamp'].min():.2f}s to {df_skeletons['timestamp'].max():.2f}s\")\n",
        "print(f\"Average frequency: {len(df_skeletons) / (df_skeletons['timestamp'].max() - df_skeletons['timestamp'].min()):.1f} Hz\")\n",
        "\n",
        "df_skeletons.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Interactive Skeleton Selection with Visualization\n",
        "\n",
        "Visualize both detected skeletons with the robot to allow selection of the \"leader\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate skeletons by ID\n",
        "unique_skeleton_ids = sorted(df_skeletons['skeleton_id'].unique())\n",
        "print(f\"Detected skeleton IDs: {unique_skeleton_ids}\")\n",
        "\n",
        "# Get a sample frame (middle of recording) for visualization\n",
        "mid_time = (df_skeletons['timestamp'].min() + df_skeletons['timestamp'].max()) / 2\n",
        "sample_frame = df_skeletons.iloc[(df_skeletons['timestamp'] - mid_time).abs().argsort()[:2]]\n",
        "\n",
        "print(f\"\\nSample frame for visualization (t={mid_time:.2f}s):\")\n",
        "print(sample_frame[['timestamp', 'skeleton_id', 'sk_0_x', 'sk_0_y', 'sk_0_z']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_skeleton_and_robot(\n",
        "    skeleton_df: pd.DataFrame,\n",
        "    robot_joint_angles: Dict[str, float],\n",
        "    fk: UR10ForwardKinematics,\n",
        "    title: str = \"Skeleton Selection\"\n",
        ") -> go.Figure:\n",
        "    \"\"\"\n",
        "    Create 3D plot showing skeletons and robot for selection.\n",
        "    \n",
        "    Args:\n",
        "        skeleton_df: DataFrame with skeleton data (can have multiple skeleton_ids)\n",
        "        robot_joint_angles: Dictionary of robot joint angles\n",
        "        fk: Forward kinematics instance\n",
        "        title: Plot title\n",
        "    \n",
        "    Returns:\n",
        "        Plotly figure\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "    \n",
        "    # Plot robot\n",
        "    robot_positions = np.array(fk.get_joint_positions(robot_joint_angles))\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=robot_positions[:, 0],\n",
        "        y=robot_positions[:, 1],\n",
        "        z=robot_positions[:, 2],\n",
        "        mode='lines+markers',\n",
        "        line=dict(color='gray', width=8),\n",
        "        marker=dict(size=8, color='gray'),\n",
        "        name='UR10 Robot',\n",
        "        showlegend=True\n",
        "    ))\n",
        "    \n",
        "    # Plot each skeleton\n",
        "    colors = ['red', 'blue', 'green', 'orange']  # For up to 4 skeletons\n",
        "    \n",
        "    for skeleton_id in skeleton_df['skeleton_id'].unique():\n",
        "        skel_row = skeleton_df[skeleton_df['skeleton_id'] == skeleton_id].iloc[0]\n",
        "        \n",
        "        # Extract keypoint positions\n",
        "        keypoints = []\n",
        "        for i in range(16):\n",
        "            x = skel_row[f'sk_{i}_x']\n",
        "            y = skel_row[f'sk_{i}_y']\n",
        "            z = skel_row[f'sk_{i}_z']\n",
        "            keypoints.append([x, y, z])\n",
        "        \n",
        "        keypoints = np.array(keypoints)\n",
        "        \n",
        "        # Plot keypoints\n",
        "        color_idx = int(skeleton_id) % len(colors)\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=keypoints[:, 0],\n",
        "            y=keypoints[:, 1],\n",
        "            z=keypoints[:, 2],\n",
        "            mode='markers',\n",
        "            marker=dict(size=6, color=colors[color_idx]),\n",
        "            name=f'Skeleton {skeleton_id}',\n",
        "            showlegend=True\n",
        "        ))\n",
        "        \n",
        "        # Define skeleton connections (ZED 16-point mode)\n",
        "        connections = [\n",
        "            (0, 1), (1, 2), (2, 3),  # Spine\n",
        "            (3, 4), (4, 5), (5, 6), (6, 7),  # Left arm\n",
        "            (3, 8), (8, 9), (9, 10), (10, 11),  # Right arm\n",
        "            (0, 12), (12, 13),  # Left leg\n",
        "            (0, 14), (14, 15)  # Right leg\n",
        "        ]\n",
        "        \n",
        "        # Plot connections\n",
        "        for start, end in connections:\n",
        "            fig.add_trace(go.Scatter3d(\n",
        "                x=[keypoints[start, 0], keypoints[end, 0]],\n",
        "                y=[keypoints[start, 1], keypoints[end, 1]],\n",
        "                z=[keypoints[start, 2], keypoints[end, 2]],\n",
        "                mode='lines',\n",
        "                line=dict(color=colors[color_idx], width=3),\n",
        "                showlegend=False\n",
        "            ))\n",
        "    \n",
        "    # Set layout\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        scene=dict(\n",
        "            xaxis_title='X (m)',\n",
        "            yaxis_title='Y (m)',\n",
        "            zaxis_title='Z (m)',\n",
        "            aspectmode='data'\n",
        "        ),\n",
        "        width=900,\n",
        "        height=700\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "\n",
        "# Get robot joint angles at sample time\n",
        "sample_joints = df_joints.iloc[(df_joints['timestamp'] - mid_time).abs().argsort()[0]]\n",
        "sample_joint_angles = {\n",
        "    fk.joint_names[i]: sample_joints[f'j{i}']\n",
        "    for i in range(6)\n",
        "}\n",
        "\n",
        "# Create visualization\n",
        "fig = plot_skeleton_and_robot(sample_frame, sample_joint_angles, fk, \n",
        "                              f\"Skeleton Selection (t={mid_time:.2f}s)\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MANUAL SELECTION - Which skeleton is the leader?\n",
        "# =============================================================================\n",
        "\n",
        "SELECTED_SKELETON_ID = 0  # <-- CHANGE THIS AFTER VIEWING VISUALIZATION\n",
        "\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"Selected skeleton ID: {SELECTED_SKELETON_ID}\")\n",
        "\n",
        "# Filter to selected skeleton only\n",
        "df_skeleton_selected = df_skeletons[df_skeletons['skeleton_id'] == SELECTED_SKELETON_ID].copy()\n",
        "df_skeleton_selected = df_skeleton_selected.drop(columns=['skeleton_id'])\n",
        "df_skeleton_selected = df_skeleton_selected.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nSelected skeleton data shape: {df_skeleton_selected.shape}\")\n",
        "print(f\"Time range: {df_skeleton_selected['timestamp'].min():.2f}s to {df_skeleton_selected['timestamp'].max():.2f}s\")\n",
        "\n",
        "# Validate selection exists throughout recording\n",
        "time_gaps = df_skeleton_selected['timestamp'].diff()\n",
        "max_gap = time_gaps.max()\n",
        "print(f\"\\nMax time gap in selected skeleton: {max_gap:.3f}s\")\n",
        "if max_gap > 0.5:\n",
        "    print(\"WARNING: Large gaps detected - skeleton tracking may be inconsistent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Synchronize Skeleton and Joint Data\n",
        "\n",
        "Interpolate joint states to skeleton timestamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def synchronize_data(\n",
        "    df_skeleton: pd.DataFrame,\n",
        "    df_joints: pd.DataFrame,\n",
        "    tolerance_sec: float = 0.05\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Synchronize skeleton and joint data by interpolating joints to skeleton timestamps.\n",
        "    \n",
        "    Args:\n",
        "        df_skeleton: Skeleton DataFrame with timestamp column\n",
        "        df_joints: Joint DataFrame with timestamp column\n",
        "        tolerance_sec: Maximum time difference for synchronization\n",
        "    \n",
        "    Returns:\n",
        "        Synchronized DataFrame with both skeleton and joint data\n",
        "    \"\"\"\n",
        "    # Interpolate each joint column to skeleton timestamps\n",
        "    joint_cols = [f'j{i}' for i in range(6)]\n",
        "    \n",
        "    interpolated_joints = {}\n",
        "    for col in joint_cols:\n",
        "        interpolated_joints[col] = np.interp(\n",
        "            df_skeleton['timestamp'],\n",
        "            df_joints['timestamp'],\n",
        "            df_joints[col]\n",
        "        )\n",
        "    \n",
        "    # Combine skeleton and interpolated joints\n",
        "    df_synced = df_skeleton.copy()\n",
        "    for col, values in interpolated_joints.items():\n",
        "        df_synced[col] = values\n",
        "    \n",
        "    return df_synced\n",
        "\n",
        "\n",
        "print(\"Synchronizing skeleton and joint data...\")\n",
        "df_synced = synchronize_data(df_skeleton_selected, df_joints)\n",
        "\n",
        "print(f\"\\nSynchronized data shape: {df_synced.shape}\")\n",
        "print(f\"Columns: {list(df_synced.columns)}\")\n",
        "print(f\"\\nSample rows:\")\n",
        "print(df_synced[['timestamp', 'sk_0_x', 'sk_0_y', 'sk_0_z', 'j0', 'j1', 'j2']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Transform Skeleton to Robot Frame\n",
        "\n",
        "Apply static transform from camera frame to robot_base_link frame.\n",
        "\n",
        "**Note:** This requires the calibrated transform. For now, we'll use a placeholder.\n",
        "In production, load from `config/camera_to_robot_transform.yaml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load static transform (if available)\n",
        "# For now, use identity transform as placeholder\n",
        "# TODO: Load from calibration file\n",
        "\n",
        "def apply_transform_to_skeleton(\n",
        "    df: pd.DataFrame,\n",
        "    translation: np.ndarray,\n",
        "    rotation_matrix: np.ndarray\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Apply rigid transform to all skeleton keypoints.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with skeleton keypoints (sk_i_x, sk_i_y, sk_i_z)\n",
        "        translation: 3D translation vector\n",
        "        rotation_matrix: 3x3 rotation matrix\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with transformed skeleton keypoints\n",
        "    \"\"\"\n",
        "    df_transformed = df.copy()\n",
        "    \n",
        "    for i in range(16):\n",
        "        # Extract keypoint\n",
        "        x = df[f'sk_{i}_x'].values\n",
        "        y = df[f'sk_{i}_y'].values\n",
        "        z = df[f'sk_{i}_z'].values\n",
        "        \n",
        "        # Apply rotation and translation\n",
        "        keypoints = np.stack([x, y, z], axis=1)  # [N, 3]\n",
        "        keypoints_transformed = (rotation_matrix @ keypoints.T).T + translation\n",
        "        \n",
        "        # Update DataFrame\n",
        "        df_transformed[f'sk_{i}_x'] = keypoints_transformed[:, 0]\n",
        "        df_transformed[f'sk_{i}_y'] = keypoints_transformed[:, 1]\n",
        "        df_transformed[f'sk_{i}_z'] = keypoints_transformed[:, 2]\n",
        "    \n",
        "    return df_transformed\n",
        "\n",
        "\n",
        "# Placeholder transform (identity)\n",
        "# TODO: Replace with actual calibrated transform\n",
        "camera_to_robot_translation = np.array([0.0, 0.0, 0.0])\n",
        "camera_to_robot_rotation = np.eye(3)\n",
        "\n",
        "print(\"Applying coordinate transform to skeleton...\")\n",
        "print(\"WARNING: Using identity transform (placeholder)\")\n",
        "print(\"TODO: Load calibrated transform from config\")\n",
        "\n",
        "df_synced_transformed = apply_transform_to_skeleton(\n",
        "    df_synced,\n",
        "    camera_to_robot_translation,\n",
        "    camera_to_robot_rotation\n",
        ")\n",
        "\n",
        "print(\"\\nTransform applied successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quality_checks(df: pd.DataFrame, fk: UR10ForwardKinematics) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Perform quality checks on synchronized data.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with quality metrics\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # 1. Check for NaN values\n",
        "    metrics['nan_count'] = df.isna().sum().sum()\n",
        "    \n",
        "    # 2. Check for position jumps in skeleton\n",
        "    pelvis_positions = df[['sk_0_x', 'sk_0_y', 'sk_0_z']].values\n",
        "    pelvis_velocity = np.linalg.norm(np.diff(pelvis_positions, axis=0), axis=1)\n",
        "    time_diffs = np.diff(df['timestamp'].values)\n",
        "    pelvis_speed = pelvis_velocity / time_diffs\n",
        "    \n",
        "    metrics['max_pelvis_speed'] = pelvis_speed.max()\n",
        "    metrics['mean_pelvis_speed'] = pelvis_speed.mean()\n",
        "    metrics['jumps_over_2ms'] = (pelvis_speed > 2.0).sum()  # 2 m/s threshold\n",
        "    \n",
        "    # 3. Check joint limits\n",
        "    joint_limit_violations = 0\n",
        "    for i, joint_name in enumerate(fk.joint_names):\n",
        "        limits = fk.joint_info[joint_name]['limits']\n",
        "        joint_values = df[f'j{i}'].values\n",
        "        violations = ((joint_values < limits[0]) | (joint_values > limits[1])).sum()\n",
        "        joint_limit_violations += violations\n",
        "    \n",
        "    metrics['joint_limit_violations'] = joint_limit_violations\n",
        "    \n",
        "    # 4. Check temporal consistency\n",
        "    metrics['num_samples'] = len(df)\n",
        "    metrics['duration_sec'] = df['timestamp'].max() - df['timestamp'].min()\n",
        "    metrics['avg_frequency_hz'] = metrics['num_samples'] / metrics['duration_sec']\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "print(\"Performing quality checks...\\n\")\n",
        "quality_metrics = quality_checks(df_synced_transformed, fk)\n",
        "\n",
        "print(\"Quality Metrics:\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in quality_metrics.items():\n",
        "    print(f\"{key:30s}: {value}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Warnings\n",
        "if quality_metrics['nan_count'] > 0:\n",
        "    print(f\"\\nWARNING: {quality_metrics['nan_count']} NaN values detected\")\n",
        "\n",
        "if quality_metrics['jumps_over_2ms'] > 0:\n",
        "    print(f\"\\nWARNING: {quality_metrics['jumps_over_2ms']} large position jumps detected\")\n",
        "\n",
        "if quality_metrics['joint_limit_violations'] > 0:\n",
        "    print(f\"\\nWARNING: {quality_metrics['joint_limit_violations']} joint limit violations detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine next recording ID\n",
        "if metadata_file.exists():\n",
        "    df_metadata = pd.read_csv(metadata_file)\n",
        "    next_rec_id = df_metadata['recording_id'].max() + 1\n",
        "else:\n",
        "    next_rec_id = 1\n",
        "    df_metadata = pd.DataFrame()\n",
        "\n",
        "rec_name = f\"rec_{next_rec_id:03d}\"\n",
        "csv_output_path = csv_output_dir / f\"{rec_name}.csv\"\n",
        "\n",
        "print(f\"Recording ID: {next_rec_id}\")\n",
        "print(f\"Output CSV: {csv_output_path}\")\n",
        "\n",
        "# Reorder columns for CSV export\n",
        "skeleton_cols = [f'sk_{i}_{axis}' for i in range(16) for axis in ['x', 'y', 'z']]\n",
        "joint_cols = [f'j{i}' for i in range(6)]\n",
        "export_cols = ['timestamp'] + skeleton_cols + joint_cols\n",
        "\n",
        "df_export = df_synced_transformed[export_cols]\n",
        "\n",
        "# Save CSV\n",
        "df_export.to_csv(csv_output_path, index=False)\n",
        "print(f\"\\nExported {len(df_export)} rows to {csv_output_path}\")\n",
        "print(f\"CSV shape: {df_export.shape}\")\n",
        "print(f\"CSV size: {csv_output_path.stat().st_size / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Update Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create metadata entry\n",
        "metadata_entry = {\n",
        "    'recording_id': next_rec_id,\n",
        "    'csv_filename': f\"{rec_name}.csv\",\n",
        "    'rosbag_name': ROSBAG_NAME,\n",
        "    'selected_skeleton_id': SELECTED_SKELETON_ID,\n",
        "    'num_samples': len(df_export),\n",
        "    'duration_sec': quality_metrics['duration_sec'],\n",
        "    'avg_frequency_hz': quality_metrics['avg_frequency_hz'],\n",
        "    'processed_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'quality_nan_count': quality_metrics['nan_count'],\n",
        "    'quality_jumps': quality_metrics['jumps_over_2ms'],\n",
        "    'quality_joint_violations': quality_metrics['joint_limit_violations']\n",
        "}\n",
        "\n",
        "# Append to metadata file\n",
        "df_metadata_new = pd.DataFrame([metadata_entry])\n",
        "if metadata_file.exists():\n",
        "    df_metadata = pd.concat([df_metadata, df_metadata_new], ignore_index=True)\n",
        "else:\n",
        "    df_metadata = df_metadata_new\n",
        "\n",
        "df_metadata.to_csv(metadata_file, index=False)\n",
        "print(f\"\\nUpdated metadata file: {metadata_file}\")\n",
        "print(f\"\\nMetadata entry:\")\n",
        "print(df_metadata_new.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"PHASE 2 PROCESSING COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nRosbag: {ROSBAG_NAME}\")\n",
        "print(f\"Recording ID: {next_rec_id}\")\n",
        "print(f\"Selected Skeleton: {SELECTED_SKELETON_ID}\")\n",
        "print(f\"\\nOutput:\")\n",
        "print(f\"  CSV: {csv_output_path}\")\n",
        "print(f\"  Rows: {len(df_export)}\")\n",
        "print(f\"  Columns: {len(df_export.columns)}\")\n",
        "print(f\"  Duration: {quality_metrics['duration_sec']:.2f}s\")\n",
        "print(f\"  Frequency: {quality_metrics['avg_frequency_hz']:.1f} Hz\")\n",
        "\n",
        "print(f\"\\nQuality:\")\n",
        "print(f\"  NaN values: {quality_metrics['nan_count']}\")\n",
        "print(f\"  Position jumps: {quality_metrics['jumps_over_2ms']}\")\n",
        "print(f\"  Joint violations: {quality_metrics['joint_limit_violations']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"NEXT STEPS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "1. Change ROSBAG_NAME at the top of this notebook to process the next rosbag\n",
        "2. Run the notebook again to process additional recordings\n",
        "3. Once all rosbags are processed, proceed to:\n",
        "   - 02_prepare_training_data.ipynb: Create PyTorch datasets\n",
        "   - 03_train_vam.ipynb: Train the Vision-Action Model\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
